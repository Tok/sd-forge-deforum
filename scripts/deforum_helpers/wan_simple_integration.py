#!/usr/bin/env python3
"""
Wan Simple Integration - Direct Model Loading Without Complex Dependencies
A simpler, more reliable approach that directly loads Wan models
"""

import os
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Any
import json
from PIL import Image
import tempfile
import time
from datetime import datetime

class WanSimpleIntegration:
    """Simple, robust Wan integration that directly loads models"""
    
    def __init__(self):
        self.extension_root = Path(__file__).parent.parent.parent
        self.discovered_models = []
        self.pipeline = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def discover_models(self) -> List[Dict]:
        """Discover Wan models using our discovery system"""
        try:
            from .wan_model_discovery import WanModelDiscovery
        except ImportError:
            # Handle running as standalone script
            import sys
            sys.path.append(str(Path(__file__).parent))
            from wan_model_discovery import WanModelDiscovery
        
        discovery = WanModelDiscovery()
        self.discovered_models = discovery.discover_models()
        return self.discovered_models
    
    def get_best_model(self) -> Optional[Dict]:
        """Get the best available model"""
        if not self.discovered_models:
            self.discover_models()
        return self.discovered_models[0] if self.discovered_models else None
    
    def test_wan_setup(self) -> bool:
        """Test if Wan setup is working properly"""
        try:
            print("ğŸ§ª Testing Wan setup...")
            
            # Check if models are available
            models = self.discover_models()
            if not models:
                print("âŒ No Wan models found")
                return False
            
            print(f"âœ… Found {len(models)} Wan models")
            
            # Try to load a model
            best_model = models[0]
            print(f"ğŸ”§ Testing model loading: {best_model['name']}")
            
            # Test model validation
            if not self._validate_wan_model(best_model):
                print("âŒ Model validation failed")
                return False
            
            print("âœ… Model validation passed")
            
            # Test pipeline creation (but don't actually load the heavy model)
            try:
                pipeline = self._create_custom_wan_pipeline(best_model)
                print("âœ… Pipeline creation test passed")
                return True
            except Exception as e:
                print(f"âŒ Pipeline creation failed: {e}")
                return False
                
        except Exception as e:
            print(f"âŒ Wan setup test failed: {e}")
            return False
    
    def load_simple_wan_pipeline(self, model_info: Dict) -> bool:
        """Load a simple Wan pipeline using the discovered model"""
        try:
            print(f"ğŸš€ Loading Wan pipeline: {model_info['name']}")
            print(f"   ğŸ“ Path: {model_info['path']}")
            print(f"   ğŸ·ï¸ Type: {model_info['type']}")
            print(f"   ğŸ“ Size: {model_info['size']}")
            
            # Validate model files first
            if not self._validate_wan_model(model_info):
                raise RuntimeError("Model validation failed")
            
            # Create custom pipeline based on model type
            self.pipeline = self._create_custom_wan_pipeline(model_info)
            
            if self.pipeline:
                print(f"âœ… Wan pipeline loaded successfully")
                return True
            else:
                print(f"âŒ Failed to create Wan pipeline")
                return False
                
        except Exception as e:
            print(f"âŒ Failed to load Wan pipeline: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _create_custom_wan_pipeline(self, model_info: Dict):
        """Create a custom Wan pipeline that handles the model correctly"""
        
        class CustomWanPipeline:
            def __init__(self, model_path: str, device: str):
                self.model_path = model_path
                self.device = device
                self.model = None
                self.loaded = False
                
                print(f"ğŸ”§ Initializing custom Wan pipeline for {model_path}")
                
                # Try to load the actual Wan model
                self._load_wan_model()
            
            def _load_wan_model(self):
                """Load the actual Wan model with multiple fallback strategies"""
                try:
                    # Strategy 1: Try to import and use official Wan
                    print("ğŸ”„ Attempting to load official Wan model...")
                    
                    # Add Wan2.1 to path if it exists
                    import sys
                    from pathlib import Path
                    
                    wan_repo_path = Path(__file__).parent.parent.parent / "Wan2.1"
                    if wan_repo_path.exists() and str(wan_repo_path) not in sys.path:
                        sys.path.insert(0, str(wan_repo_path))
                        print(f"ğŸ“ Added Wan repo to path: {wan_repo_path}")
                    
                    # Try importing Wan components
                    try:
                        import wan  # type: ignore
                        from wan.text2video import WanT2V  # type: ignore
                        from wan.image2video import WanI2V  # type: ignore
                        
                        print("âœ… Official Wan modules imported successfully")
                        
                        # Create minimal config if needed
                        try:
                            from wan.configs.wan_t2v_14B import t2v_14B as t2v_config
                            from wan.configs.wan_i2v_14B import i2v_14B as i2v_config
                            print("âœ… Loaded Wan configs")
                        except ImportError:
                            print("âš ï¸ Config files not found, creating minimal configs...")
                            # Create minimal config structure
                            class MinimalConfig:
                                def __init__(self):
                                    self.model = type('obj', (object,), {
                                        'num_attention_heads': 32,
                                        'attention_head_dim': 128,
                                        'in_channels': 4,
                                        'out_channels': 4,
                                        'num_layers': 28,
                                        'sample_size': 32,
                                        'patch_size': 2,
                                        'num_vector_embeds': None,
                                        'activation_fn': "geglu",
                                        'num_embeds_ada_norm': 1000,
                                        'norm_elementwise_affine': False,
                                        'norm_eps': 1e-6,
                                        'attention_bias': True,
                                        'caption_channels': 4096
                                    })
                                    
                            t2v_config = MinimalConfig()
                            i2v_config = MinimalConfig()
                        
                        # Initialize T2V model with correct parameters
                        self.t2v_model = WanT2V(
                            config=t2v_config,
                            checkpoint_dir=self.model_path,
                            device_id=0,
                            rank=0,
                            dit_fsdp=False,
                            t5_fsdp=False
                        )
                        
                        # Try to initialize I2V model if available
                        try:
                            self.i2v_model = WanI2V(
                                config=i2v_config,
                                checkpoint_dir=self.model_path,
                                device_id=0,
                                rank=0,
                                dit_fsdp=False,
                                t5_fsdp=False
                            )
                            print("âœ… I2V model loaded for chaining support")
                        except Exception as e:
                            print(f"âš ï¸ I2V model not available: {e}")
                            self.i2v_model = None
                        
                        self.loaded = True
                        print("âœ… Official Wan models loaded successfully")
                        return
                        
                    except ImportError as e:
                        print(f"âŒ Official Wan import failed: {e}")
                        raise RuntimeError(f"""
âŒ CRITICAL: Official Wan repository not found or not properly installed!

ğŸ”§ REQUIRED SETUP:
1. ğŸ“¥ Clone the official Wan repository:
   git clone https://github.com/Wan-Video/Wan2.1.git

2. ğŸ“¦ Install Wan dependencies:
   cd Wan2.1
   pip install -e .

3. ğŸ“¥ Install Flash Attention (required):
   pip install flash-attn --no-build-isolation

4. ğŸ“‚ Download Wan models:
   huggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir models/wan

5. âœ… Restart WebUI after setup

ğŸ’¡ Wan requires the official repository - no fallbacks available.
""")
                    
                    # Strategy 2: Try diffusers-based loading
                    try:
                        from diffusers import DiffusionPipeline
                        
                        print("ğŸ”„ Attempting diffusers-based loading...")
                        self.model = DiffusionPipeline.from_pretrained(
                            self.model_path,
                            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                            device_map="auto" if torch.cuda.is_available() else None
                        )
                        
                        if torch.cuda.is_available():
                            self.model = self.model.to(self.device)
                        
                        self.loaded = True
                        print("âœ… Diffusers-based model loaded successfully")
                        return
                        
                    except Exception as e:
                        print(f"âŒ Diffusers loading failed: {e}")
                        raise RuntimeError(f"""
âŒ CRITICAL: Failed to load Wan model with diffusers!

ğŸ”§ TROUBLESHOOTING:
1. âœ… Verify model files are complete in: {self.model_path}
2. ğŸ“¦ Install required dependencies:
   pip install diffusers transformers accelerate
3. ğŸ”„ Try re-downloading the model:
   huggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir models/wan
4. ğŸ’¾ Check available disk space and memory

ğŸ’¡ Model loading failed - check the error above for details.
Error: {e}
""")
                    
                except Exception as e:
                    print(f"âŒ All model loading strategies failed: {e}")
                    import traceback
                    traceback.print_exc()
                    raise RuntimeError(f"""
âŒ CRITICAL: Wan model loading completely failed!

ğŸ”§ COMPLETE SETUP GUIDE:
1. ğŸ“¥ Clone official Wan repository:
   git clone https://github.com/Wan-Video/Wan2.1.git

2. ğŸ“¦ Install Wan with dependencies:
   cd Wan2.1
   pip install -e .
   pip install flash-attn --no-build-isolation

3. ğŸ“‚ Download models to correct location:
   huggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir models/wan

4. âœ… Verify model files exist:
   - models/wan/diffusion_pytorch_model.safetensors
   - models/wan/config.json
   - models/wan/Wan2.1_VAE.pth
   - models/wan/models_t5_umt5-xxl-enc-bf16.pth

5. ğŸ”„ Restart WebUI completely

âŒ NO FALLBACKS AVAILABLE - Real Wan implementation required!
Error: {e}
""")
            
            def __call__(self, prompt, height, width, num_frames, num_inference_steps, guidance_scale, **kwargs):
                """Generate video frames"""
                if not self.loaded:
                    raise RuntimeError("Wan model not loaded")
                
                print(f"ğŸ¬ Generating {num_frames} frames with Wan...")
                print(f"   ğŸ“ Prompt: {prompt[:50]}...")
                print(f"   ğŸ“ Size: {width}x{height}")
                print(f"   ğŸ”§ Steps: {num_inference_steps}")
                print(f"   ğŸ“ Guidance: {guidance_scale}")
                
                try:
                    # Use official Wan T2V if available
                    if hasattr(self, 't2v_model') and self.t2v_model:
                        print("ğŸš€ Using official Wan T2V model")
                        
                        result = self.t2v_model.generate(
                            input_prompt=prompt,
                            size=(width, height),
                            frame_num=num_frames,
                            sampling_steps=num_inference_steps,
                            guide_scale=guidance_scale,
                            shift=5.0,
                            sample_solver='unipc',
                            offload_model=True,
                            **kwargs
                        )
                        
                        return result
                    
                    # Use diffusers model if available
                    elif hasattr(self, 'model') and self.model:
                        print("ğŸš€ Using diffusers-based model")
                        
                        generation_kwargs = {
                            "prompt": prompt,
                            "height": height,
                            "width": width,
                            "num_frames": num_frames,
                            "num_inference_steps": num_inference_steps,
                            "guidance_scale": guidance_scale,
                        }
                        
                        # Add optional parameters if supported
                        import inspect
                        if hasattr(self.model, '__call__'):
                            sig = inspect.signature(self.model.__call__)
                            
                            if 'output_type' in sig.parameters:
                                generation_kwargs['output_type'] = 'pil'
                            if 'return_dict' in sig.parameters:
                                generation_kwargs['return_dict'] = False
                        
                        with torch.no_grad():
                            result = self.model(**generation_kwargs)
                        
                        return result
                    
                    # No valid model available
                    else:
                        raise RuntimeError("""
âŒ CRITICAL: No valid Wan model loaded!

ğŸ”§ SETUP REQUIRED:
1. Install official Wan repository
2. Download Wan models
3. Restart WebUI

ğŸ’¡ No fallbacks available - real Wan implementation required.
""")
                
                except Exception as e:
                    print(f"âŒ Generation failed: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    raise RuntimeError(f"""
âŒ CRITICAL: Wan video generation failed!

ğŸ”§ TROUBLESHOOTING:
1. âœ… Verify Wan models are properly installed
2. ğŸ”„ Check CUDA/GPU availability: {torch.cuda.is_available()}
3. ğŸ’¾ Check available VRAM/memory
4. ğŸ“¦ Verify all dependencies are installed

âŒ NO FALLBACKS - Real Wan implementation required!
Error: {e}
""")
            
            def generate_image2video(self, image, prompt, height, width, num_frames, num_inference_steps, guidance_scale, **kwargs):
                """Generate video from image (I2V)"""
                if not self.loaded:
                    raise RuntimeError("Wan model not loaded")
                
                print(f"ğŸ¬ Generating I2V {num_frames} frames with Wan...")
                
                try:
                    # Use official Wan I2V if available
                    if hasattr(self, 'i2v_model') and self.i2v_model:
                        print("ğŸš€ Using official Wan I2V model")
                        
                        result = self.i2v_model.generate(
                            input_prompt=prompt,
                            img=image,
                            max_area=height * width,
                            frame_num=num_frames,
                            sampling_steps=num_inference_steps,
                            guide_scale=guidance_scale,
                            shift=5.0,
                            sample_solver='unipc',
                            offload_model=True,
                            **kwargs
                        )
                        
                        return result
                    
                    # Fallback to T2V with enhanced prompt if I2V not available
                    else:
                        print("âš ï¸ I2V model not available, using enhanced T2V")
                        enhanced_prompt = f"Starting from the given image, {prompt}. Maintain visual continuity."
                        
                        return self.__call__(
                            enhanced_prompt, height, width, num_frames, 
                            num_inference_steps, guidance_scale, **kwargs
                        )
                
                except Exception as e:
                    print(f"âŒ I2V generation failed: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    raise RuntimeError(f"""
âŒ CRITICAL: Wan I2V generation failed!

ğŸ”§ TROUBLESHOOTING:
1. âœ… Verify I2V model is properly loaded
2. ğŸ–¼ï¸ Check input image format and size
3. ğŸ’¾ Check available VRAM/memory
4. ğŸ“¦ Verify Wan I2V dependencies

âŒ NO FALLBACKS - Real Wan I2V implementation required!
Error: {e}
""")
        
        # Create and return the custom pipeline
        return CustomWanPipeline(model_info['path'], self.device)
    
    def _validate_wan_model(self, model_info: Dict) -> bool:
        """Validate Wan model has required files"""
        model_path = Path(model_info['path'])
        
        # Check for required files
        required_files = [
            "diffusion_pytorch_model.safetensors",
            "config.json"
        ]
        
        # Check for VAE and T5 files (different naming conventions)
        vae_files = ["Wan2.1_VAE.pth", "vae.pth", "vae.safetensors"]
        t5_files = ["models_t5_umt5-xxl-enc-bf16.pth", "t5.pth", "t5.safetensors"]
        
        missing_files = []
        
        for file in required_files:
            if not (model_path / file).exists():
                missing_files.append(file)
        
        # Check for at least one VAE file
        if not any((model_path / vae).exists() for vae in vae_files):
            missing_files.append("VAE file (Wan2.1_VAE.pth, vae.pth, or vae.safetensors)")
            
        # Check for at least one T5 file  
        if not any((model_path / t5).exists() for t5 in t5_files):
            missing_files.append("T5 file (models_t5_umt5-xxl-enc-bf16.pth, t5.pth, or t5.safetensors)")
        
        if missing_files:
            print(f"âŒ Missing required model files: {missing_files}")
            return False
            
        print("âœ… All required Wan model files found")
        return True
    
    def _generate_with_wan_pipeline(self, 
                                  prompt: str,
                                  width: int,
                                  height: int,
                                  num_frames: int,
                                  steps: int,
                                  guidance_scale: float,
                                  seed: int,
                                  output_path: str) -> bool:
        """Generate video using the loaded Wan pipeline"""
        try:
            if not self.pipeline:
                raise RuntimeError("Wan pipeline not loaded")
            
            print(f"ğŸ¬ Running Wan inference...")
            print(f"   ğŸ“ Prompt: {prompt[:50]}...")
            print(f"   ğŸ“ Size: {width}x{height}")
            print(f"   ğŸ¬ Frames: {num_frames}")
            print(f"   ğŸ”§ Steps: {steps}")
            print(f"   ğŸ“ Guidance: {guidance_scale}")
            
            # Set seed for reproducibility
            if seed > 0:
                torch.manual_seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(seed)
            
            # Prepare generation parameters
            generation_kwargs = {
                "prompt": prompt,
                "height": height,
                "width": width,
                "num_frames": num_frames,
                "num_inference_steps": steps,
                "guidance_scale": guidance_scale,
            }
            
            # Add additional parameters if the pipeline supports them
            if hasattr(self.pipeline, '__call__'):
                import inspect
                sig = inspect.signature(self.pipeline.__call__)
                
                # Add seed if supported
                if 'generator' in sig.parameters and seed > 0:
                    generator = torch.Generator(device=self.device)
                    generator.manual_seed(seed)
                    generation_kwargs['generator'] = generator
                
                # Add other common parameters
                if 'output_type' in sig.parameters:
                    generation_kwargs['output_type'] = 'pil'
                if 'return_dict' in sig.parameters:
                    generation_kwargs['return_dict'] = False
            
            print("ğŸš€ Starting Wan model inference...")
            
            # Generate the video
            with torch.no_grad():
                result = self.pipeline(**generation_kwargs)
            
            # Handle different result formats
            if isinstance(result, tuple):
                frames = result[0]
            elif hasattr(result, 'frames'):
                frames = result.frames
            elif isinstance(result, list):
                frames = result
            else:
                frames = result
            
            # Save frames as video
            self._save_frames_as_video(frames, output_path, fps=8)
            
            print(f"âœ… Wan video saved to: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Wan pipeline generation failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Wan pipeline generation failed: {e}")
    
    def _save_frames_as_video(self, frame_paths_or_frames, output_path: str, fps: int = 8):
        """Save frames as video file - handles both PNG paths and frame arrays"""
        try:
            import imageio
            import numpy as np
            from PIL import Image
            
            # Check if we have file paths or frame data
            if isinstance(frame_paths_or_frames, list) and len(frame_paths_or_frames) > 0:
                if isinstance(frame_paths_or_frames[0], str):
                    # We have PNG file paths
                    print(f"ğŸ’¾ Creating video from {len(frame_paths_or_frames)} PNG files...")
                    
                    processed_frames = []
                    for i, frame_path in enumerate(frame_paths_or_frames):
                        # Load PNG file
                        pil_image = Image.open(frame_path)
                        frame_np = np.array(pil_image)
                        
                        # Ensure RGB format
                        if len(frame_np.shape) == 2:  # Grayscale
                            frame_np = np.stack([frame_np, frame_np, frame_np], axis=2)
                        elif len(frame_np.shape) == 3 and frame_np.shape[2] == 4:  # RGBA
                            frame_np = frame_np[:, :, :3]  # Remove alpha
                        
                        processed_frames.append(frame_np)
                        
                        if i % 10 == 0:
                            print(f"  ğŸ“– Loaded frame {i+1}/{len(frame_paths_or_frames)}")
                    
                    print(f"ğŸ¬ Saving {len(processed_frames)} frames to {output_path}")
                    imageio.mimsave(output_path, processed_frames, fps=fps, format='mp4')
                    print(f"âœ… Video saved successfully with {len(processed_frames)} frames at {fps} FPS")
                    return
            
            # Fallback to original frame array handling
            frames = frame_paths_or_frames
            print(f"ğŸ’¾ Saving video with {len(frames) if hasattr(frames, '__len__') else 'unknown'} frames...")
            
            # Handle tensor format conversion
            if isinstance(frames, torch.Tensor):
                print(f"ğŸ”„ Converting tensor with shape: {frames.shape}")
                frames_np = frames.cpu().numpy()
                
                # Handle different tensor formats
                if len(frames_np.shape) == 4:  # (C, F, H, W) or (F, H, W, C)
                    if frames_np.shape[0] == 3:  # (C, F, H, W) - channels first
                        print("ğŸ”„ Converting from (C, F, H, W) to (F, H, W, C)")
                        frames_np = frames_np.transpose(1, 2, 3, 0)  # (F, H, W, C)
                    # else assume (F, H, W, C) already
                
                processed_frames = []
                for i in range(frames_np.shape[0]):
                    frame = frames_np[i]  # (H, W, C)
                    
                    # Ensure 3 channels
                    if len(frame.shape) == 2:  # Grayscale
                        frame = np.stack([frame, frame, frame], axis=2)
                    elif len(frame.shape) == 3 and frame.shape[2] == 1:  # (H, W, 1)
                        frame = np.repeat(frame, 3, axis=2)
                    elif len(frame.shape) == 3 and frame.shape[2] > 3:  # Too many channels
                        frame = frame[:, :, :3]
                    
                    # Convert to uint8
                    if frame.dtype != np.uint8:
                        if frame.max() <= 1.0:
                            frame = (frame * 255).astype(np.uint8)
                        else:
                            frame = np.clip(frame, 0, 255).astype(np.uint8)
                    
                    processed_frames.append(frame)
            
            else:
                # Handle list of frames
                processed_frames = []
                for i, frame in enumerate(frames):
                    if hasattr(frame, 'cpu'):
                        frame_np = frame.cpu().numpy()
                    elif isinstance(frame, np.ndarray):
                        frame_np = frame
                    else:
                        frame_np = np.array(frame)
                    
                    # Ensure correct format (H, W, C)
                    if len(frame_np.shape) == 4:  # (B, H, W, C)
                        frame_np = frame_np[0]
                    if len(frame_np.shape) == 3 and frame_np.shape[0] == 3:  # (C, H, W)
                        frame_np = frame_np.transpose(1, 2, 0)
                    
                    # Ensure 3 channels
                    if len(frame_np.shape) == 2:  # Grayscale
                        frame_np = np.stack([frame_np, frame_np, frame_np], axis=2)
                    elif len(frame_np.shape) == 3 and frame_np.shape[2] == 1:  # (H, W, 1)
                        frame_np = np.repeat(frame_np, 3, axis=2)
                    elif len(frame_np.shape) == 3 and frame_np.shape[2] > 3:  # Too many channels
                        frame_np = frame_np[:, :, :3]
                    
                    # Convert to uint8
                    if frame_np.dtype != np.uint8:
                        if frame_np.max() <= 1.0:
                            frame_np = (frame_np * 255).astype(np.uint8)
                        else:
                            frame_np = np.clip(frame_np, 0, 255).astype(np.uint8)
                    
                    processed_frames.append(frame_np)
            
            print(f"ğŸ¬ Saving {len(processed_frames)} frames to {output_path}")
            
            # Save as video
            imageio.mimsave(output_path, processed_frames, fps=fps, format='mp4')
            print(f"âœ… Video saved successfully with {len(processed_frames)} frames at {fps} FPS")
            
        except Exception as e:
            print(f"âŒ Failed to save video: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Failed to save video: {e}")
    
    def generate_video_simple(self, 
                            prompt: str,
                            model_info: Dict,
                            output_dir: str,
                            width: int = 1280,
                            height: int = 720,
                            num_frames: int = 81,
                            steps: int = 20,
                            guidance_scale: float = 7.5,
                            seed: int = -1,
                            **kwargs) -> Optional[str]:
        """Generate video using simple Wan integration with PNG frame output and I2V chaining"""
        
        print(f"ğŸ¬ Generating video using SIMPLE Wan integration...")
        print(f"   ğŸ“ Prompt: {prompt}")
        print(f"   ğŸ“ Size: {width}x{height}")
        print(f"   ğŸ¬ Frames: {num_frames}")
        print(f"   ğŸ“ Model: {model_info['name']} ({model_info['type']}, {model_info['size']})")
        
        # Calculate Wan frames (4n+1 rule) and show frame discarding info
        wan_frames = self._calculate_wan_frames(num_frames)
        discard_info = self._calculate_frame_discarding(num_frames, wan_frames)
        
        print(f"ğŸ“Š Wan Frame Calculation:")
        print(f"   ğŸ¯ Requested frames: {num_frames}")
        print(f"   ğŸ”§ Wan frames (4n+1): {wan_frames}")
        print(f"   ğŸ—‘ï¸ Frames to discard: {discard_info['discard_count']}")
        if discard_info['discard_count'] > 0:
            print(f"   ğŸ“ Discarded frame range: {discard_info['discard_start']}-{discard_info['discard_end']}")
        
        # Validate model first
        if not self._validate_wan_model(model_info):
            raise RuntimeError("Wan model validation failed - missing required files")
        
        # Load the model if not loaded
        if not self.pipeline:
            self.load_simple_wan_pipeline(model_info)  # This will raise if it fails
        
        try:
            # Ensure output directory exists
            os.makedirs(output_dir, exist_ok=True)
            
            # Generate PNG frames instead of video
            frames_dir = os.path.join(output_dir, "wan_frames")
            os.makedirs(frames_dir, exist_ok=True)
            
            print("ğŸ¬ Generating PNG frames with Wan model...")
            
            # Generate frames using Wan
            frames = self._generate_wan_frames(
                prompt=prompt,
                width=width,
                height=height,
                num_frames=wan_frames,
                steps=steps,
                guidance_scale=guidance_scale,
                seed=seed,
                frames_dir=frames_dir
            )
            
            if frames:
                # Apply frame discarding if needed
                final_frames = self._apply_frame_discarding(frames, discard_info)
                
                print(f"âœ… Generated {len(final_frames)} final frames (discarded {len(frames) - len(final_frames)})")
                
                # Save frame paths for potential I2V chaining
                frame_paths = self._save_frame_paths(final_frames, frames_dir)
                
                # Create video from final frames
                output_filename = f"wan_video_{int(time.time())}.mp4"
                output_path = os.path.join(output_dir, output_filename)
                
                self._save_frames_as_video(final_frames, output_path, fps=8)
                
                print(f"âœ… Wan video generated: {output_path}")
                return output_path
            else:
                raise RuntimeError("Wan frame generation returned no result")
                
        except Exception as e:
            print(f"âŒ Wan video generation failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Wan video generation failed: {e}")
    
    def _calculate_wan_frames(self, requested_frames: int) -> int:
        """Calculate the number of frames Wan should generate (4n+1 rule)"""
        # Wan requires frames to be 4n+1
        # Find the smallest 4n+1 that is >= requested_frames
        if (requested_frames - 1) % 4 == 0:
            return requested_frames  # Already follows 4n+1
        else:
            n = (requested_frames - 1) // 4 + 1
            return 4 * n + 1
    
    def _calculate_frame_discarding(self, requested_frames: int, wan_frames: int) -> Dict:
        """Calculate which frames to discard to match requested count"""
        discard_count = wan_frames - requested_frames
        
        if discard_count <= 0:
            return {
                'discard_count': 0,
                'discard_start': None,
                'discard_end': None,
                'keep_indices': list(range(wan_frames))
            }
        
        # Discard from the middle to preserve start and end frames
        start_keep = wan_frames // 3  # Keep first third
        end_keep = wan_frames - (wan_frames // 3)  # Keep last third
        
        # Calculate actual discard range
        discard_start = start_keep
        discard_end = discard_start + discard_count
        
        # Ensure we don't go out of bounds
        if discard_end > end_keep:
            discard_end = end_keep
            discard_start = discard_end - discard_count
        
        # Create list of indices to keep
        keep_indices = list(range(discard_start)) + list(range(discard_end, wan_frames))
        
        return {
            'discard_count': discard_count,
            'discard_start': discard_start,
            'discard_end': discard_end - 1,  # Make it inclusive
            'keep_indices': keep_indices[:requested_frames]  # Ensure exact count
        }
    
    def _generate_wan_frames(self,
                           prompt: str,
                           width: int,
                           height: int,
                           num_frames: int,
                           steps: int,
                           guidance_scale: float,
                           seed: int,
                           frames_dir: str) -> List:
        """Generate frames using Wan and save as PNGs"""
        try:
            if not self.pipeline:
                raise RuntimeError("Wan pipeline not loaded")
            
            print(f"ğŸ¬ Running Wan inference for {num_frames} frames...")
            print(f"   ğŸ“ Prompt: {prompt[:50]}...")
            print(f"   ğŸ“ Size: {width}x{height}")
            print(f"   ğŸ”§ Steps: {steps}")
            print(f"   ğŸ“ Guidance: {guidance_scale}")
            
            # Set seed for reproducibility
            if seed > 0:
                torch.manual_seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(seed)
                print(f"   ğŸ² Seed: {seed}")
            
            # Prepare generation parameters
            generation_kwargs = {
                "prompt": prompt,
                "height": height,
                "width": width,
                "num_frames": num_frames,
                "num_inference_steps": steps,
                "guidance_scale": guidance_scale,
            }
            
            # Add seed if supported
            if hasattr(self.pipeline, '__call__'):
                import inspect
                sig = inspect.signature(self.pipeline.__call__)
                
                if 'generator' in sig.parameters and seed > 0:
                    generator = torch.Generator(device=self.device)
                    generator.manual_seed(seed)
                    generation_kwargs['generator'] = generator
                
                if 'output_type' in sig.parameters:
                    generation_kwargs['output_type'] = 'pil'
                if 'return_dict' in sig.parameters:
                    generation_kwargs['return_dict'] = False
            
            print("ğŸš€ Starting Wan frame generation...")
            
            # Generate the frames
            with torch.no_grad():
                result = self.pipeline(**generation_kwargs)
            
            # Handle different result formats
            if isinstance(result, tuple):
                frames = result[0]
            elif hasattr(result, 'frames'):
                frames = result.frames
            elif isinstance(result, list):
                frames = result
            else:
                frames = result
            
            print(f"ğŸ¬ Wan generation completed, processing {len(frames) if hasattr(frames, '__len__') else 'unknown'} frames...")
            
            # Save frames as PNGs
            saved_frames = self._save_frames_as_pngs(frames, frames_dir)
            
            print(f"âœ… Generated and saved {len(saved_frames)} PNG frames")
            return saved_frames
            
        except Exception as e:
            print(f"âŒ Wan frame generation failed: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Wan frame generation failed: {e}")
    
    def _save_frames_as_pngs(self, frames, frames_dir: str) -> List[str]:
        """Save frames as individual PNG files with improved error handling"""
        try:
            import imageio
            import numpy as np
            from PIL import Image
            
            saved_paths = []
            
            print(f"ğŸ’¾ Saving frames as PNGs to {frames_dir}...")
            
            # Handle tensor format conversion
            if isinstance(frames, torch.Tensor):
                print(f"ğŸ”„ Converting tensor with shape: {frames.shape}")
                frames_np = frames.cpu().numpy()
                
                # Handle different tensor formats
                if len(frames_np.shape) == 4:  # (C, F, H, W) or (F, H, W, C)
                    if frames_np.shape[0] == 3:  # (C, F, H, W) - channels first
                        print("ğŸ”„ Converting from (C, F, H, W) to (F, H, W, C)")
                        frames_np = frames_np.transpose(1, 2, 3, 0)  # (F, H, W, C)
                    # else assume (F, H, W, C) already
                elif len(frames_np.shape) == 5:  # (B, F, C, H, W) or (B, F, H, W, C)
                    print(f"ğŸ”„ Converting 5D tensor: {frames_np.shape}")
                    frames_np = frames_np[0]  # Remove batch dimension
                    if frames_np.shape[1] == 3:  # (F, C, H, W)
                        frames_np = frames_np.transpose(0, 2, 3, 1)  # (F, H, W, C)
                
                # Debug: Check tensor value range and normalize if needed
                print(f"ğŸ¨ Tensor value range: min={frames_np.min():.3f}, max={frames_np.max():.3f}")
                print(f"ğŸ¨ Tensor dtype: {frames_np.dtype}")
                
                # Normalize to [0, 1] if needed (Wan often outputs in [-1, 1] range)
                if frames_np.min() < 0 and frames_np.max() <= 1:
                    print("ğŸ”„ Normalizing from [-1, 1] to [0, 1] range")
                    frames_np = (frames_np + 1.0) / 2.0
                    frames_np = np.clip(frames_np, 0, 1)
                
                processed_frames = []
                for i in range(frames_np.shape[0]):
                    frame = frames_np[i]  # (H, W, C)
                    
                    # Ensure 3 channels
                    if len(frame.shape) == 2:  # Grayscale
                        frame = np.stack([frame, frame, frame], axis=2)
                    elif len(frame.shape) == 3 and frame.shape[2] == 1:  # (H, W, 1)
                        frame = np.repeat(frame, 3, axis=2)
                    elif len(frame.shape) == 3 and frame.shape[2] > 3:  # Too many channels
                        frame = frame[:, :, :3]
                    
                    # Convert to uint8
                    if frame.dtype != np.uint8:
                        if frame.max() <= 1.0:
                            frame = (frame * 255).astype(np.uint8)
                        else:
                            frame = np.clip(frame, 0, 255).astype(np.uint8)
                    
                    # Wan outputs RGB format consistently for both T2V and I2V
                    # No BGR conversion needed
                    processed_frames.append(frame)
            
            else:
                # Handle list of frames or PIL images
                processed_frames = []
                for i, frame in enumerate(frames):
                    if hasattr(frame, 'cpu'):  # PyTorch tensor
                        frame_np = frame.cpu().numpy()
                    elif isinstance(frame, np.ndarray):
                        frame_np = frame
                    elif hasattr(frame, 'save'):  # PIL Image
                        frame_np = np.array(frame)
                    else:
                        frame_np = np.array(frame)
                    
                    # Ensure correct format (H, W, C)
                    if len(frame_np.shape) == 4:  # (B, H, W, C)
                        frame_np = frame_np[0]
                    if len(frame_np.shape) == 3 and frame_np.shape[0] == 3:  # (C, H, W)
                        frame_np = frame_np.transpose(1, 2, 0)
                    
                    # Ensure 3 channels
                    if len(frame_np.shape) == 2:  # Grayscale
                        frame_np = np.stack([frame_np, frame_np, frame_np], axis=2)
                    elif len(frame_np.shape) == 3 and frame_np.shape[2] == 1:  # (H, W, 1)
                        frame_np = np.repeat(frame_np, 3, axis=2)
                    elif len(frame_np.shape) == 3 and frame_np.shape[2] > 3:  # Too many channels
                        frame_np = frame_np[:, :, :3]
                    
                    # Convert to uint8
                    if frame_np.dtype != np.uint8:
                        if frame_np.max() <= 1.0:
                            frame_np = (frame_np * 255).astype(np.uint8)
                        else:
                            frame_np = np.clip(frame_np, 0, 255).astype(np.uint8)
                    
                    # Wan outputs RGB format consistently for both T2V and I2V
                    # No BGR conversion needed
                    processed_frames.append(frame_np)
            
            # Save each frame as PNG
            for i, frame in enumerate(processed_frames):
                frame_path = os.path.join(frames_dir, f"frame_{i:06d}.png")
                
                # Convert to PIL Image and save
                pil_image = Image.fromarray(frame)
                pil_image.save(frame_path, "PNG")
                
                saved_paths.append(frame_path)
                
                if i % 10 == 0:
                    print(f"  ğŸ’¾ Saved frame {i+1}/{len(processed_frames)}")
            
            print(f"âœ… All {len(saved_paths)} frames saved as PNGs")
            return saved_paths
            
        except Exception as e:
            print(f"âŒ Failed to save frames as PNGs: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Failed to save frames as PNGs: {e}")
    
    def _apply_frame_discarding(self, frame_paths: List[str], discard_info: Dict) -> List[str]:
        """Apply frame discarding based on the calculated discard info"""
        if discard_info['discard_count'] <= 0:
            return frame_paths
        
        keep_indices = discard_info['keep_indices']
        final_frames = [frame_paths[i] for i in keep_indices if i < len(frame_paths)]
        
        print(f"ğŸ—‘ï¸ Discarded {len(frame_paths) - len(final_frames)} frames to match requested count")
        return final_frames
    
    def _save_frame_paths(self, frame_paths: List[str], frames_dir: str) -> str:
        """Save frame paths to a text file for potential I2V chaining"""
        paths_file = os.path.join(frames_dir, "frame_paths.txt")
        
        with open(paths_file, 'w') as f:
            for path in frame_paths:
                f.write(f"{path}\n")
        
        print(f"ğŸ“ Frame paths saved to: {paths_file}")
        return paths_file
    
    def unload_model(self):
        """Unload the model to free memory"""
        if self.pipeline:
            try:
                if hasattr(self.pipeline, 'to'):
                    self.pipeline.to('cpu')
                del self.pipeline
                self.pipeline = None
                
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
                print("ğŸ§¹ Wan model unloaded, memory freed")
            except Exception as e:
                print(f"âš ï¸ Error unloading model: {e}")
    
    def generate_video_with_i2v_chaining(self, 
                                       clips: List[Dict],
                                       model_info: Dict,
                                       output_dir: str,
                                       width: int = 1280,
                                       height: int = 720,
                                       steps: int = 20,
                                       guidance_scale: float = 7.5,
                                       seed: int = -1,
                                       anim_args=None,
                                       **kwargs) -> Optional[str]:
        """Generate video using I2V chaining for better continuity between clips - with unified frame output and strength scheduling"""
        try:
            import shutil
            import os
            from datetime import datetime
            
            print(f"ğŸ¬ Starting I2V chained generation with {len(clips)} clips...")
            print(f"ğŸ“ Model: {model_info['name']} ({model_info['type']}, {model_info['size']})")
            
            # Parse strength schedule if available
            strength_values = {}
            if anim_args and hasattr(anim_args, 'strength_schedule'):
                try:
                    # Parse the strength schedule (format: "0: (0.85), 60: (0.7)")
                    import re
                    strength_schedule = anim_args.strength_schedule
                    print(f"ğŸ¯ Using Deforum strength schedule: {strength_schedule}")
                    
                    # Extract frame:value pairs
                    matches = re.findall(r'(\d+):\s*\(([0-9.]+)\)', strength_schedule)
                    for frame_str, strength_str in matches:
                        frame_num = int(frame_str)
                        strength_val = float(strength_str)
                        strength_values[frame_num] = strength_val
                        
                    print(f"ğŸ“Š Parsed strength schedule: {strength_values}")
                except Exception as e:
                    print(f"âš ï¸ Failed to parse strength schedule: {e}, using default strength")
            
            # Validate model first
            if not self._validate_wan_model(model_info):
                raise RuntimeError("Wan model validation failed - missing required files")
            
            # Load the model if not loaded
            if not self.pipeline:
                print("ğŸ”§ Loading Wan pipeline for I2V chaining...")
                if not self.load_simple_wan_pipeline(model_info):
                    raise RuntimeError("Failed to load Wan pipeline")
            
            # Get the timestring from the output directory name or create one
            timestring = os.path.basename(output_dir).split('_')[-1]
            if not timestring or len(timestring) != 14:
                # Fallback: extract from directory name or create new
                dir_parts = os.path.basename(output_dir).split('_')
                for part in dir_parts:
                    if len(part) == 14 and part.isdigit():
                        timestring = part
                        break
                else:
                    # Create new timestring if none found
                    timestring = datetime.now().strftime("%Y%m%d%H%M%S")
            
            print(f"ğŸ“ Output directory: {output_dir}")
            print(f"ğŸ• Using timestring: {timestring}")
            
            # Create unified frames directory
            unified_frames_dir = output_dir
            os.makedirs(unified_frames_dir, exist_ok=True)
            
            all_frame_paths = []
            total_frame_idx = 0
            last_frame_path = None
            
            for clip_idx, clip in enumerate(clips):
                print(f"\nğŸ¬ Generating clip {clip_idx + 1}/{len(clips)}")
                print(f"   ğŸ“ Prompt: {clip['prompt'][:50]}...")
                print(f"   ğŸï¸ Frames: {clip['num_frames']}")
                
                # Calculate strength for this clip based on its start frame
                clip_strength = 0.85  # Default strength
                if strength_values:
                    # Find the appropriate strength value for this clip's start frame
                    clip_start_frame = clip['start_frame']
                    
                    # Find the closest strength value at or before this frame
                    applicable_frames = [f for f in strength_values.keys() if f <= clip_start_frame]
                    if applicable_frames:
                        closest_frame = max(applicable_frames)
                        clip_strength = strength_values[closest_frame]
                        print(f"   ğŸ’ª Using strength {clip_strength} (from frame {closest_frame} schedule)")
                    else:
                        # Use the first strength value if no earlier frame found
                        first_frame = min(strength_values.keys())
                        clip_strength = strength_values[first_frame]
                        print(f"   ğŸ’ª Using strength {clip_strength} (from first scheduled frame {first_frame})")
                else:
                    print(f"   ğŸ’ª Using default strength {clip_strength}")
                
                # Create temporary directory for this clip
                temp_clip_dir = os.path.join(output_dir, f"_temp_clip_{clip_idx:03d}")
                os.makedirs(temp_clip_dir, exist_ok=True)
                
                # Calculate frame discarding for this clip
                wan_frames = self._calculate_wan_frames(clip['num_frames'])
                discard_info = self._calculate_frame_discarding(clip['num_frames'], wan_frames)
                
                print(f"ğŸ¯ Wan will generate {wan_frames} frames, targeting {clip['num_frames']} final frames")
                if discard_info['discard_count'] > 0:
                    print(f"ğŸ—‘ï¸ Will discard {discard_info['discard_count']} frames from the middle to preserve start/end frames")
                
                # Generate frames for this clip
                if clip_idx == 0 or last_frame_path is None:
                    # First clip: use T2V
                    print("ğŸš€ Using T2V for first clip")
                    clip_frames = self._generate_wan_frames(
                        prompt=clip['prompt'],
                        width=width,
                        height=height,
                        num_frames=wan_frames,
                        steps=steps,
                        guidance_scale=guidance_scale,
                        seed=seed if seed > 0 else -1,
                        frames_dir=temp_clip_dir
                    )
                else:
                    # Subsequent clips: use I2V with last frame and strength scheduling
                    print(f"ğŸ”— Using I2V chaining from: {os.path.basename(last_frame_path)}")
                    print(f"ğŸ’ª I2V strength: {clip_strength} (controls influence of previous frame)")
                    clip_frames = self._generate_wan_i2v_frames(
                        prompt=clip['prompt'],
                        init_image_path=last_frame_path,
                        width=width,
                        height=height,
                        num_frames=wan_frames,
                        steps=steps,
                        guidance_scale=guidance_scale,
                        seed=seed if seed > 0 else -1,
                        frames_dir=temp_clip_dir,
                        strength=clip_strength  # Pass strength to I2V generation
                    )
                
                if not clip_frames:
                    raise RuntimeError(f"Failed to generate frames for clip {clip_idx + 1}")
                
                print(f"âœ… Generated {len(clip_frames)} I2V frames")
                
                # Apply frame discarding
                final_clip_frames = self._apply_frame_discarding(clip_frames, discard_info)
                
                print(f"âœ… Generated {len(final_clip_frames)} final frames for clip {clip_idx + 1}")
                
                # Copy frames to unified directory with continuous numbering and proper naming
                import shutil
                for frame_idx, src_path in enumerate(final_clip_frames):
                    # Use Deforum's naming convention: timestring_000000000.png
                    dst_filename = f"{timestring}_{total_frame_idx:09d}.png"
                    dst_path = os.path.join(unified_frames_dir, dst_filename)
                    
                    # Copy the frame
                    shutil.copy2(src_path, dst_path)
                    all_frame_paths.append(dst_path)
                    
                    total_frame_idx += 1
                
                # Clean up temporary clip directory
                shutil.rmtree(temp_clip_dir, ignore_errors=True)
                
                # Update last frame for next clip
                if final_clip_frames:
                    last_frame_path = all_frame_paths[-1]  # Use the copied frame path
                    print(f"ğŸ”— Last frame for next clip: {last_frame_path}")
            
            print(f"\nâœ… All clips generated! Total frames: {len(all_frame_paths)}")
            print(f"ğŸ“ All frames saved to: {unified_frames_dir}")
            
            if strength_values:
                print(f"ğŸ’ª Strength scheduling was applied across {len(clips)} clips")
            
            # No need to create video here - Deforum will handle it with ffmpeg
            # Just return the output directory
            return unified_frames_dir
            
        except Exception as e:
            print(f"âŒ I2V chained video generation failed: {e}")
            raise RuntimeError(f"I2V chained video generation failed: {e}")
    
    def _generate_wan_i2v_frames(self,
                               prompt: str,
                               init_image_path: str,
                               width: int,
                               height: int,
                               num_frames: int,
                               steps: int,
                               guidance_scale: float,
                               seed: int,
                               frames_dir: str,
                               strength: float = 0.85) -> List[str]:
        """Generate frames using Wan I2V (Image-to-Video) mode with strength control"""
        try:
            from PIL import Image
            
            print(f"ğŸ¬ Running Wan I2V inference for {num_frames} frames...")
            print(f"   ğŸ–¼ï¸ Init image: {init_image_path}")
            print(f"   ğŸ“ Prompt: {prompt[:50]}...")
            print(f"   ğŸ“ Size: {width}x{height}")
            print(f"   ğŸ’ª Strength: {strength} (influence of init image)")
            
            # Load and prepare the initial image
            if not os.path.exists(init_image_path):
                raise RuntimeError(f"Init image not found: {init_image_path}")
                
            init_image = Image.open(init_image_path)
            
            # Resize to match target dimensions if needed
            if init_image.size != (width, height):
                init_image = init_image.resize((width, height), Image.Resampling.LANCZOS)
                print(f"ğŸ”„ Resized init image from {init_image.size} to {width}x{height}")
            
            # Set seed for reproducibility
            if seed > 0:
                torch.manual_seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(seed)
                print(f"   ğŸ² Seed: {seed}")
            
            # Try different I2V approaches based on available pipeline methods
            frames = None
            
            # Method 1: Check if pipeline has dedicated I2V method
            if hasattr(self.pipeline, 'generate_image2video'):
                print("ğŸš€ Using dedicated I2V pipeline method")
                
                generation_kwargs = {
                    "image": init_image,
                    "prompt": prompt,
                    "height": height,
                    "width": width,
                    "num_frames": num_frames,
                    "num_inference_steps": steps,
                    "guidance_scale": guidance_scale,
                }
                
                # Add strength parameter if supported
                import inspect
                sig = inspect.signature(self.pipeline.generate_image2video)
                if 'strength' in sig.parameters:
                    generation_kwargs['strength'] = strength
                    print(f"âœ… Added strength parameter: {strength}")
                elif 'image_guidance_scale' in sig.parameters:
                    # Some I2V models use image_guidance_scale instead of strength
                    # Convert strength to image guidance scale (inverse relationship)
                    image_guidance = guidance_scale * (1.0 - strength)
                    generation_kwargs['image_guidance_scale'] = image_guidance
                    print(f"âœ… Using image_guidance_scale: {image_guidance} (derived from strength {strength})")
                else:
                    print(f"âš ï¸ Strength parameter not supported by this I2V method")
                
                # Add seed if supported
                if seed > 0 and 'generator' in sig.parameters:
                    generator = torch.Generator(device=self.device)
                    generator.manual_seed(seed)
                    generation_kwargs['generator'] = generator
                
                with torch.no_grad():
                    result = self.pipeline.generate_image2video(**generation_kwargs)
                
                # Handle result format
                if isinstance(result, tuple):
                    frames = result[0]
                elif hasattr(result, 'frames'):
                    frames = result.frames
                elif isinstance(result, list):
                    frames = result
                else:
                    frames = result
            
            # Method 2: Check if pipeline supports image conditioning in main call
            elif hasattr(self.pipeline, '__call__'):
                import inspect
                sig = inspect.signature(self.pipeline.__call__)
                
                if 'image' in sig.parameters or 'init_image' in sig.parameters:
                    print("ğŸš€ Using main pipeline with image conditioning")
                    
                    generation_kwargs = {
                        "prompt": prompt,
                        "height": height,
                        "width": width,
                        "num_frames": num_frames,
                        "num_inference_steps": steps,
                        "guidance_scale": guidance_scale,
                    }
                    
                    # Add image parameter
                    if 'image' in sig.parameters:
                        generation_kwargs['image'] = init_image
                    elif 'init_image' in sig.parameters:
                        generation_kwargs['init_image'] = init_image
                    
                    # Add strength parameter if supported
                    if 'strength' in sig.parameters:
                        generation_kwargs['strength'] = strength
                        print(f"âœ… Added strength parameter: {strength}")
                    elif 'image_guidance_scale' in sig.parameters:
                        image_guidance = guidance_scale * (1.0 - strength)
                        generation_kwargs['image_guidance_scale'] = image_guidance
                        print(f"âœ… Using image_guidance_scale: {image_guidance}")
                    
                    # Add seed if supported
                    if 'generator' in sig.parameters and seed > 0:
                        generator = torch.Generator(device=self.device)
                        generator.manual_seed(seed)
                        generation_kwargs['generator'] = generator
                    
                    with torch.no_grad():
                        result = self.pipeline(**generation_kwargs)
                    
                    # Handle result format
                    if isinstance(result, tuple):
                        frames = result[0]
                    elif hasattr(result, 'frames'):
                        frames = result.frames
                    elif isinstance(result, list):
                        frames = result
                    else:
                        frames = result
                else:
                    print("âš ï¸ Pipeline doesn't support image conditioning, using enhanced T2V")
                    frames = None
            
            # Method 3: Fallback to enhanced T2V with image-aware prompt
            if frames is None:
                print("ğŸ”„ Using enhanced T2V with image-aware prompt as I2V fallback")
                
                # Create a more detailed prompt that references the starting image and strength
                if strength > 0.7:
                    continuity_desc = "maintaining strong visual continuity from the previous scene"
                elif strength > 0.4:
                    continuity_desc = "with moderate visual continuity from the previous scene"
                else:
                    continuity_desc = "with subtle visual continuity from the previous scene"
                
                enhanced_prompt = f"Continuing from the previous scene, {prompt}. {continuity_desc}."
                
                generation_kwargs = {
                    "prompt": enhanced_prompt,
                    "height": height,
                    "width": width,
                    "num_frames": num_frames,
                    "num_inference_steps": steps,
                    "guidance_scale": guidance_scale,
                }
                
                # Add seed if supported
                if hasattr(self.pipeline, '__call__'):
                    import inspect
                    sig = inspect.signature(self.pipeline.__call__)
                    
                    if 'generator' in sig.parameters and seed > 0:
                        generator = torch.Generator(device=self.device)
                        generator.manual_seed(seed)
                        generation_kwargs['generator'] = generator
                    
                    if 'output_type' in sig.parameters:
                        generation_kwargs['output_type'] = 'pil'
                    if 'return_dict' in sig.parameters:
                        generation_kwargs['return_dict'] = False
                
                with torch.no_grad():
                    result = self.pipeline(**generation_kwargs)
                
                # Handle result format
                if isinstance(result, tuple):
                    frames = result[0]
                elif hasattr(result, 'frames'):
                    frames = result.frames
                elif isinstance(result, list):
                    frames = result
                else:
                    frames = result
            
            if frames is None:
                raise RuntimeError("All I2V methods failed to generate frames")
            
            print(f"ğŸ¬ I2V generation completed, processing {len(frames) if hasattr(frames, '__len__') else 'unknown'} frames...")
            
            # Save frames as PNGs
            saved_frames = self._save_frames_as_pngs(frames, frames_dir)
            
            print(f"âœ… Generated and saved {len(saved_frames)} I2V PNG frames with strength {strength}")
            return saved_frames
            
        except Exception as e:
            print(f"âŒ Wan I2V frame generation failed: {e}")
            print(f"ğŸ”„ Falling back to T2V generation...")
            import traceback
            traceback.print_exc()
            
            # Fallback to T2V if I2V fails
            return self._generate_wan_frames(
                prompt=prompt,
                width=width,
                height=height,
                num_frames=num_frames,
                steps=steps,
                guidance_scale=guidance_scale,
                seed=seed,
                frames_dir=frames_dir
            )

def generate_video_with_simple_wan(prompt: str, 
                                 output_dir: str,
                                 width: int = 1280,
                                 height: int = 720,
                                 num_frames: int = 81,
                                 steps: int = 20,
                                 guidance_scale: float = 7.5,
                                 seed: int = -1,
                                 **kwargs) -> str:
    """Convenience function to generate video using Wan - fail fast if no models"""
    
    integration = WanSimpleIntegration()
    
    # Find the best model
    best_model = integration.get_best_model()
    if not best_model:
        raise RuntimeError("""âŒ No Wan models found!

ğŸ’¡ SOLUTION: Download a Wan model first:
   huggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir ./models/wan

Then restart generation.""")
    
    # Generate video - let errors propagate
    try:
        result = integration.generate_video_simple(
            prompt=prompt,
            model_info=best_model,
            output_dir=output_dir,
            width=width,
            height=height,
            num_frames=num_frames,
            steps=steps,
            guidance_scale=guidance_scale,
            seed=seed,
            **kwargs
        )
        
        return result
        
    finally:
        # Always clean up
        integration.unload_model()

if __name__ == "__main__":
    # Test the simple integration
    print("ğŸ§ª Testing Wan Simple Integration...")
    
    integration = WanSimpleIntegration()
    
    # Test model discovery
    models = integration.discover_models()
    if models:
        print(f"âœ… Found {len(models)} model(s)")
        best = integration.get_best_model()
        print(f"ğŸ† Best model: {best['name']} ({best['type']}, {best['size']})")
        
        # Test simple loading
        if integration.load_simple_wan_pipeline(best):
            print("âœ… Simple Wan integration ready")
            
            # Test demo generation
            output = integration.generate_video_simple(
                prompt="A beautiful landscape",
                model_info=best,
                output_dir="./test_output",
                num_frames=10
            )
            
            if output:
                print(f"âœ… Demo generation successful: {output}")
            else:
                print("âŒ Demo generation failed")
        else:
            print("âŒ Failed to load Wan models")
    else:
        print("âŒ No models found")

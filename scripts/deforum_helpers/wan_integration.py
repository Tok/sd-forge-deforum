"""
True WAN 2.1 Integration Module - Uses Actual WAN Models
Loads and uses the existing WAN model files (60GB+) with proper T5/VAE/CLIP handling
"""

import os
import torch
import numpy as np
from PIL import Image
from typing import List, Tuple, Optional, Dict, Any
import json
from pathlib import Path
import sys
import subprocess
import safetensors
from safetensors import safe_open


class WanVideoGenerator:
    """
    Real WAN 2.1 video generator using actual WAN model files
    """
    
    def __init__(self, model_path: str, device: str = "cuda"):
        self.model_path = Path(model_path)
        self.device = device
        self.loaded = False
        self.wan_model = None
        self.t5_encoder = None
        self.vae = None
        self.clip_encoder = None
        
    def validate_model_structure(self) -> Dict[str, Any]:
        """
        Validate WAN model files and download missing components
        """
        print("ðŸ” Validating WAN model structure...")
        
        # Check for DiT model files (these are present)
        dit_files = list(self.model_path.glob("diffusion_pytorch_model-*.safetensors"))
        
        validation_result = {
            'dit_files': len(dit_files),
            'dit_size_gb': sum(f.stat().st_size for f in dit_files) / (1024**3),
            'missing_components': [],
            'can_use_wan': False
        }
        
        print(f"ðŸ“ Found {len(dit_files)} DiT model files ({validation_result['dit_size_gb']:.1f}GB)")
        
        # Check for T5 encoder
        t5_files = [
            'models_t5_umt5-xxl-enc-bf16.pth',
            'umt5-xxl-enc-bf16.safetensors', 
            'models_t5_umt5-xxl-enc-bf16.safetensors'
        ]
        
        t5_found = any((self.model_path / f).exists() for f in t5_files)
        if not t5_found:
            validation_result['missing_components'].append('t5_encoder')
            
        # Check for VAE
        vae_files = [
            'Wan2.1_VAE.pth',
            'wan_2.1_vae.safetensors',
            'Wan2_1_VAE_fp32.safetensors'
        ]
        
        vae_found = any((self.model_path / f).exists() for f in vae_files)
        if not vae_found:
            validation_result['missing_components'].append('vae')
            
        # Check for CLIP
        clip_files = [
            'models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth',
            'clip_vision_h.safetensors'
        ]
        
        clip_found = any((self.model_path / f).exists() for f in clip_files)
        if not clip_found:
            validation_result['missing_components'].append('clip')
        
        # Download missing components
        if validation_result['missing_components']:
            print(f"ðŸ“¥ Downloading missing components: {validation_result['missing_components']}")
            self._download_missing_components(validation_result['missing_components'])
            
        validation_result['can_use_wan'] = True
        return validation_result
    
    def _download_missing_components(self, missing: List[str]):
        """Download missing WAN components from HuggingFace"""
        
        # Install huggingface_hub if needed
        try:
            from huggingface_hub import hf_hub_download, snapshot_download
        except ImportError:
            print("ðŸ“¦ Installing huggingface_hub...")
            subprocess.run([sys.executable, "-m", "pip", "install", "huggingface_hub"], check=True)
            from huggingface_hub import hf_hub_download, snapshot_download
        
        # Download T5 encoder
        if 't5_encoder' in missing:
            print("ðŸ“¥ Downloading T5 encoder...")
            try:
                t5_path = hf_hub_download(
                    repo_id="LanguageBind/Open-Sora-Plan-v1.1.0", 
                    filename="t5-v1_1-xxl/pytorch_model.bin",
                    local_dir=str(self.model_path),
                    local_dir_use_symlinks=False
                )
                # Move and rename to expected location
                expected_path = self.model_path / "models_t5_umt5-xxl-enc-bf16.pth"
                if Path(t5_path).exists() and not expected_path.exists():
                    Path(t5_path).rename(expected_path)
                print("âœ… T5 encoder downloaded")
            except Exception as e:
                print(f"âš ï¸ T5 download failed: {e}, trying alternative...")
                # Fallback: use a smaller T5 model
                try:
                    from transformers import T5Tokenizer, T5EncoderModel
                    print("ðŸ“¥ Using Transformers T5 model as fallback...")
                    self.t5_tokenizer = T5Tokenizer.from_pretrained("t5-large")
                    self.t5_model = T5EncoderModel.from_pretrained("t5-large").to(self.device)
                    print("âœ… T5 fallback model loaded")
                except Exception as e2:
                    print(f"âŒ T5 fallback failed: {e2}")
        
        # Download VAE
        if 'vae' in missing:
            print("ðŸ“¥ Downloading VAE...")
            try:
                vae_path = hf_hub_download(
                    repo_id="Wan-AI/Wan2.1-T2V-14B",
                    filename="vae/diffusion_pytorch_model.safetensors", 
                    local_dir=str(self.model_path),
                    local_dir_use_symlinks=False
                )
                # Move to expected location
                expected_path = self.model_path / "wan_2.1_vae.safetensors"
                if Path(vae_path).exists() and not expected_path.exists():
                    Path(vae_path).rename(expected_path)
                print("âœ… VAE downloaded")
            except Exception as e:
                print(f"âš ï¸ VAE download failed: {e}")
        
        # Download CLIP
        if 'clip' in missing:
            print("ðŸ“¥ Downloading CLIP...")
            try:
                clip_path = hf_hub_download(
                    repo_id="laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
                    filename="pytorch_model.bin",
                    local_dir=str(self.model_path),
                    local_dir_use_symlinks=False
                )
                # Move to expected location  
                expected_path = self.model_path / "clip_vision_h.safetensors"
                if Path(clip_path).exists() and not expected_path.exists():
                    # Convert to safetensors if needed
                    import torch
                    state_dict = torch.load(clip_path, map_location='cpu')
                    safetensors.torch.save_file(state_dict, expected_path)
                print("âœ… CLIP downloaded and converted")
            except Exception as e:
                print(f"âš ï¸ CLIP download failed: {e}")
    
    def setup_wan_repository(self) -> Path:
        """Setup official WAN 2.1 repository"""
        print("ðŸš€ Setting up official WAN 2.1 repository...")
        
        extension_root = Path(__file__).parent.parent.parent
        wan_repo_dir = extension_root / "wan_official_repo"
        
        if not wan_repo_dir.exists() or not (wan_repo_dir / "wan" / "text2video.py").exists():
            try:
                if wan_repo_dir.exists():
                    import shutil
                    shutil.rmtree(wan_repo_dir)
                    
                result = subprocess.run([
                    "git", "clone", "--depth", "1",
                    "https://github.com/Wan-Video/Wan2.1.git",
                    str(wan_repo_dir)
                ], capture_output=True, text=True, timeout=300)
                
                if result.returncode != 0:
                    raise subprocess.CalledProcessError(result.returncode, "git clone", result.stderr)
                    
            except Exception as e:
                raise RuntimeError(f"Failed to setup WAN repository: {e}")
        
        print(f"âœ… WAN repository ready at: {wan_repo_dir}")
        return wan_repo_dir
    
    def load_model(self):
        """Load actual WAN 2.1 model"""
        if self.loaded:
            return
            
        print("ðŸ”„ Loading WAN 2.1 model...")
        
        # Validate and setup
        validation_result = self.validate_model_structure()
        wan_repo_path = self.setup_wan_repository()
        
        # Add repository to path
        if str(wan_repo_path) not in sys.path:
            sys.path.insert(0, str(wan_repo_path))
        
        try:
            # Import WAN modules
            print("ðŸ“¦ Importing WAN modules...")
            import wan.text2video as wt2v
            import wan.image2video as wi2v
            from wan.configs.wan_t2v_14B import t2v_14B as t2v_config
            from wan.configs.wan_i2v_14B import i2v_14B as i2v_config
            
            # Initialize WAN pipelines with actual models
            print("ðŸ”§ Initializing WAN T2V pipeline...")
            self.wan_t2v_pipeline = wt2v.WanT2V(
                config=t2v_config,
                checkpoint_dir=str(self.model_path),
                device_id=0,
                rank=0,
                dit_fsdp=False,
                t5_fsdp=False
            )
            
            print("ðŸ”§ Initializing WAN I2V pipeline...")
            self.wan_i2v_pipeline = wi2v.WanI2V(
                config=i2v_config, 
                checkpoint_dir=str(self.model_path),
                device_id=0,
                rank=0,
                dit_fsdp=False,
                t5_fsdp=False
            )
            
            self.loaded = True
            print("ðŸŽ‰ WAN 2.1 model loaded successfully!")
            
        except Exception as e:
            print(f"âŒ Failed to load WAN model: {e}")
            raise RuntimeError(f"WAN model loading failed: {e}")
    
    def generate_txt2video(self, 
                          prompt: str, 
                          duration: float = 4.0,
                          fps: int = 60,
                          resolution: str = "1280x720",
                          steps: int = 50,
                          guidance_scale: float = 7.5,
                          seed: int = -1,
                          motion_strength: float = 1.0,
                          **kwargs) -> List[np.ndarray]:
        """Generate video from text using real WAN 2.1"""
        
        if not self.loaded:
            self.load_model()
            
        width, height = map(int, resolution.split('x'))
        num_frames = self.calculate_frame_count(duration, fps)
        
        print(f"ðŸŽ¬ Generating text-to-video with WAN 2.1:")
        print(f"  Prompt: {prompt}")
        print(f"  Resolution: {width}x{height}")
        print(f"  Frames: {num_frames}")
        print(f"  Steps: {steps}")
        
        try:
            # Generate with actual WAN pipeline
            video_tensor = self.wan_t2v_pipeline.generate(
                input_prompt=prompt,
                size=(width, height),
                frame_num=num_frames,
                sampling_steps=steps,
                guide_scale=guidance_scale,
                seed=seed if seed != -1 else None,
                shift=5.0,
                sample_solver='unipc',
                offload_model=True
            )
            
            # Convert tensor to numpy frames
            if video_tensor is not None:
                # WAN returns (C, T, H, W) format
                video_tensor = video_tensor.cpu()
                # Normalize from [-1, 1] to [0, 255]
                video_tensor = (video_tensor + 1.0) * 127.5
                video_tensor = torch.clamp(video_tensor, 0, 255).to(torch.uint8)
                
                # Convert to (T, H, W, C) and then to list of numpy arrays
                frames = video_tensor.permute(1, 2, 3, 0).numpy()
                frame_list = [frames[i] for i in range(frames.shape[0])]
                
                # Convert RGB to BGR for consistency
                frame_list = [frame[..., ::-1] for frame in frame_list]  # RGB to BGR
                
                print(f"âœ… Generated {len(frame_list)} frames with WAN 2.1")
                return frame_list
            else:
                raise RuntimeError("WAN T2V returned None")
                
        except Exception as e:
            print(f"âŒ WAN T2V generation failed: {e}")
            raise RuntimeError(f"WAN text-to-video generation failed: {e}")
    
    def generate_img2video(self, 
                          init_image: np.ndarray,
                          prompt: str, 
                          duration: float = 4.0,
                          fps: int = 60,
                          resolution: str = "1280x720",
                          steps: int = 50,
                          guidance_scale: float = 7.5,
                          seed: int = -1,
                          motion_strength: float = 1.0,
                          **kwargs) -> List[np.ndarray]:
        """Generate video from image using real WAN 2.1"""
        
        if not self.loaded:
            self.load_model()
            
        width, height = map(int, resolution.split('x'))
        num_frames = self.calculate_frame_count(duration, fps)
        
        print(f"ðŸŽ¬ Generating image-to-video with WAN 2.1:")
        print(f"  Prompt: {prompt}")
        print(f"  Resolution: {width}x{height}")
        print(f"  Frames: {num_frames}")
        
        try:
            # Convert init image to PIL
            if init_image.dtype != np.uint8:
                init_image = (init_image * 255).astype(np.uint8)
            
            # Convert BGR to RGB for PIL
            init_pil = Image.fromarray(init_image[..., ::-1]).resize((width, height))
            
            # Generate with actual WAN I2V pipeline
            video_tensor = self.wan_i2v_pipeline.generate(
                input_prompt=prompt,
                img=init_pil,
                max_area=width * height,
                frame_num=num_frames,
                sampling_steps=steps,
                guide_scale=guidance_scale,
                seed=seed if seed != -1 else None,
                shift=5.0,
                sample_solver='unipc',
                offload_model=True
            )
            
            # Convert tensor to numpy frames
            if video_tensor is not None:
                # WAN returns (C, T, H, W) format
                video_tensor = video_tensor.cpu()
                # Normalize from [-1, 1] to [0, 255]
                video_tensor = (video_tensor + 1.0) * 127.5
                video_tensor = torch.clamp(video_tensor, 0, 255).to(torch.uint8)
                
                # Convert to (T, H, W, C) and then to list of numpy arrays
                frames = video_tensor.permute(1, 2, 3, 0).numpy()
                frame_list = [frames[i] for i in range(frames.shape[0])]
                
                # Convert RGB to BGR for consistency
                frame_list = [frame[..., ::-1] for frame in frame_list]  # RGB to BGR
                
                print(f"âœ… Generated {len(frame_list)} frames with WAN 2.1")
                return frame_list
            else:
                raise RuntimeError("WAN I2V returned None")
                
        except Exception as e:
            print(f"âŒ WAN I2V generation failed: {e}")
            raise RuntimeError(f"WAN image-to-video generation failed: {e}")
        
    def calculate_frame_count(self, duration: float, fps: float) -> int:
        """Calculate frame count for WAN (requires 4n+1 format)"""
        if duration <= 0 or fps <= 0:
            raise ValueError("Duration and FPS must be positive")
        
        frames = max(1, int(duration * fps))
        # WAN requires frame count in 4n+1 format
        adjusted_frames = ((frames - 1) // 4) * 4 + 1
        return adjusted_frames
        
    def extract_last_frame(self, video_frames: List) -> np.ndarray:
        """Extract last frame"""
        if not video_frames:
            raise ValueError("Empty video frames list")
        return video_frames[-1].copy()
        
    def unload_model(self):
        """Unload WAN model and free memory"""
        if hasattr(self, 'wan_t2v_pipeline'):
            del self.wan_t2v_pipeline
        if hasattr(self, 'wan_i2v_pipeline'):
            del self.wan_i2v_pipeline
            
        import gc
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        print("ðŸ—‘ï¸ WAN model unloaded and GPU memory freed")
        self.loaded = False


class WanPromptScheduler:
    """Handle prompt scheduling for WAN video generation"""
    
    def __init__(self, animation_prompts: Dict[str, str], wan_args, video_args):
        if not animation_prompts:
            raise ValueError("Animation prompts cannot be empty")
        self.animation_prompts = animation_prompts
        self.wan_args = wan_args
        self.video_args = video_args
        
    def parse_prompts_and_timing(self) -> List[Tuple[str, float, float]]:
        """Parse animation prompts and calculate timing"""
        frame_prompts = []
        
        for frame_str, prompt in self.animation_prompts.items():
            try:
                if isinstance(frame_str, str) and frame_str.isdigit():
                    frame_num = int(frame_str)
                elif isinstance(frame_str, (int, float)):
                    frame_num = int(frame_str)
                else:
                    continue
                frame_prompts.append((frame_num, prompt))
            except ValueError:
                continue
                
        frame_prompts.sort(key=lambda x: x[0])
        
        if not frame_prompts:
            raise ValueError("No valid frame prompts found")
            
        fps = self.wan_args.wan_fps
        default_duration = self.wan_args.wan_clip_duration
        
        clips = []
        for i, (frame_num, prompt) in enumerate(frame_prompts):
            start_time = frame_num / fps
            
            if i < len(frame_prompts) - 1:
                next_frame = frame_prompts[i + 1][0]
                frame_count = next_frame - frame_num
                duration = frame_count / fps
            else:
                duration = default_duration
            
            if i == len(frame_prompts) - 1:
                duration = min(duration, 8.0)
                
            clips.append((prompt, start_time, duration))
            
        return clips

def validate_wan_settings(wan_args) -> List[str]:
    """Validate WAN settings"""
    errors = []
    
    if wan_args.wan_enabled:
        try:
            width, height = map(int, wan_args.wan_resolution.split('x'))
            if width <= 0 or height <= 0:
                errors.append("Invalid resolution: dimensions must be positive")
        except (ValueError, AttributeError):
            errors.append(f"Invalid resolution format: {wan_args.wan_resolution}")
            
        if wan_args.wan_clip_duration <= 0 or wan_args.wan_clip_duration > 30:
            errors.append("Clip duration must be between 0 and 30 seconds")
            
        if wan_args.wan_fps <= 0 or wan_args.wan_fps > 60:
            errors.append("FPS must be between 1 and 60")
            
        if wan_args.wan_inference_steps < 1 or wan_args.wan_inference_steps > 100:
            errors.append("Inference steps must be between 1 and 100")
            
        if wan_args.wan_guidance_scale < 1.0 or wan_args.wan_guidance_scale > 20.0:
            errors.append("Guidance scale must be between 1.0 and 20.0")
    
    if errors:
        raise ValueError("WAN validation failed: " + "; ".join(errors))
    
    return []

def should_disable_setting_for_wan(setting_name: str, wan_enabled: bool) -> bool:
    """Determine if setting should be disabled for WAN mode"""
    if not wan_enabled:
        return False
        
    disabled_settings = {
        'angle', 'zoom', 'translation_x', 'translation_y', 'translation_z',
        'use_depth_warping', 'optical_flow_cadence', 'hybrid_motion',
        'diffusion_cadence', 'strength_schedule', 'color_coherence'
    }
    
    return setting_name in disabled_settings
